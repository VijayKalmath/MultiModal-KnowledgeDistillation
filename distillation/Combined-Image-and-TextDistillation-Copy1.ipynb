{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87273e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import KLDivLoss, CrossEntropyLoss, CosineEmbeddingLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd6d6",
   "metadata": {},
   "source": [
    "### Loading Teacher models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cbfadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "model_name = \"ViT-B/32\"\n",
    "\n",
    "# model is the torch model.\n",
    "# preprocess function is for image preprocessing.\n",
    "\n",
    "model, preprocess = clip.load(model_name)\n",
    "\n",
    "# Get only the visual model\n",
    "visual_teacher_model = model.visual\n",
    "text_teacher_model = model.transformer\n",
    "\n",
    "\n",
    "input_resolution = model.visual.input_resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896653c",
   "metadata": {},
   "source": [
    "### Instantiating Student models\n",
    "\n",
    "[VisionTransformer](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/model.py#L206)\n",
    "[Transformer](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/model.py#L195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1340820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip.model import Transformer, VisionTransformer\n",
    "from clip.model import convert_weights  # Make them float16\n",
    "\n",
    "# Set Student Configuration\n",
    "\n",
    "patch_size = 32\n",
    "width = 384\n",
    "layers = 6\n",
    "heads = 12\n",
    "output_dim = 512\n",
    "\n",
    "visual_student_model = VisionTransformer(\n",
    "    input_resolution=input_resolution,\n",
    "    patch_size=patch_size,\n",
    "    width=width,\n",
    "    layers=layers,\n",
    "    heads=heads,\n",
    "    output_dim=output_dim,\n",
    ")\n",
    "\n",
    "width = 512\n",
    "layers = 6\n",
    "heads = 8  # More Number of Heads\n",
    "\n",
    "\n",
    "def build_attention_mask():\n",
    "    context_length = 77\n",
    "    mask = torch.empty(context_length, context_length)\n",
    "    mask.fill_(float(\"-inf\"))\n",
    "    mask.triu_(1)  # zero out the lower diagonal\n",
    "    return mask\n",
    "\n",
    "\n",
    "text_student_model = Transformer(\n",
    "    width=width, layers=layers, heads=heads, attn_mask=build_attention_mask()\n",
    ")\n",
    "\n",
    "\n",
    "convert_weights(visual_student_model)\n",
    "convert_weights(text_student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c600678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(transformer, text):\n",
    "\n",
    "    x = model.token_embedding(text).type(model.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "    x = x + model.positional_embedding.type(model.dtype)\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "\n",
    "    x = transformer(x)\n",
    "\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "    x = model.ln_final(x).type(model.dtype)\n",
    "\n",
    "    # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "    # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "    x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ model.text_projection\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c4d724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, projection_dim=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6debb0c",
   "metadata": {},
   "source": [
    "### Load the WIT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6383d10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: conceptual_captions/unlabeled\n",
      "Reusing dataset conceptual_captions (/home/ecbm4040/.cache/huggingface/datasets/conceptual_captions/unlabeled/1.0.0/05266784888422e36944016874c44639bccb39069c2227435168ad8b02d600d8)\n",
      "Loading cached processed dataset at /home/ecbm4040/.cache/huggingface/datasets/conceptual_captions/unlabeled/1.0.0/05266784888422e36944016874c44639bccb39069c2227435168ad8b02d600d8/cache-f02225eefe1d1385.arrow\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import io\n",
    "import urllib\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "\n",
    "\n",
    "def fetch_single_image(image_url, timeout=None, retries=0):\n",
    "    for _ in range(retries + 1):\n",
    "        try:\n",
    "            request = urllib.request.Request(\n",
    "                image_url,\n",
    "                data=None,\n",
    "                headers={\"user-agent\": get_datasets_user_agent()},\n",
    "            )\n",
    "            with urllib.request.urlopen(request, timeout=timeout) as req:\n",
    "                image = PIL.Image.open(io.BytesIO(req.read()))\n",
    "            break\n",
    "        except Exception:\n",
    "            image = None\n",
    "    return image\n",
    "\n",
    "\n",
    "def fetch_images(batch, num_threads, timeout=None, retries=0):\n",
    "    fetch_single_image_with_args = partial(\n",
    "        fetch_single_image, timeout=timeout, retries=retries\n",
    "    )\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        batch[\"image\"] = list(\n",
    "            executor.map(fetch_single_image_with_args, batch[\"image_url\"])\n",
    "        )\n",
    "    return batch\n",
    "\n",
    "\n",
    "num_threads = 20\n",
    "dset = load_dataset(\"conceptual_captions\", split=\"train[:10000]\")\n",
    "dset = dset.remove_columns(\"image_url\")\n",
    "\n",
    "dset = dset.filter(lambda example: len(example[\"caption\"]) < 75)\n",
    "\n",
    "dset = dset.map(\n",
    "    fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": num_threads}\n",
    ")\n",
    "\n",
    "dset = dset.filter(lambda example: example[\"image\"] is not None)\n",
    "\n",
    "\n",
    "def transform_func(examples):\n",
    "    examples[\"image\"] = [preprocess(img) for img in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "dset = dset.with_transform(transform_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948606c",
   "metadata": {},
   "source": [
    "### Convert Dataset into Torch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0ef402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c6ecc8ad494b07aff613acc2e021d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345876aeccf14a4ab0af7280f254a83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def transform_func(examples):\n",
    "    examples[\"image\"] = [preprocess(img) for img in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "dset = dset.with_transform(transform_func)\n",
    "\n",
    "train_dataloader = DataLoader(dset, batch_size=16, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db4ba5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['caption', 'image'],\n",
       "    num_rows: 7019\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "609fd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "\n",
    "        self.visual_teacher = visual_teacher_model\n",
    "        self.text_teacher = text_teacher_model\n",
    "\n",
    "        self.visual_student = visual_student_model\n",
    "        self.text_student = text_student_model\n",
    "\n",
    "        self.image_projection = ProjectionHead().half()\n",
    "        self.text_projection = ProjectionHead().half()\n",
    "        self.temperature = 1\n",
    "\n",
    "        self.train_dataloader = train_dataloader\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.visual_teacher = self.visual_teacher.to(self.device)\n",
    "        self.text_teacher = self.text_teacher.to(self.device)\n",
    "        self.visual_student = self.visual_student.to(self.device)\n",
    "        self.text_student = self.text_student.to(self.device)\n",
    "        self.image_projection = self.image_projection.to(self.device)\n",
    "        self.text_projection = self.text_projection.to(self.device)\n",
    "\n",
    "        self.visual_teacher.eval()\n",
    "        self.text_teacher.eval()\n",
    "\n",
    "        self.epochs = 30\n",
    "        self.start_epoch = 1\n",
    "\n",
    "        # set up optimizer\n",
    "        self.optimizer = SGD(\n",
    "            list(self.visual_student.parameters())\n",
    "            + list(self.text_student.parameters()),\n",
    "            lr=0.001,\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, images, texts, return_outputs=False):\n",
    "        texts = clip.tokenize(texts)\n",
    "\n",
    "        texts = texts.to(self.device)\n",
    "\n",
    "        images = images.to(self.device).half()\n",
    "\n",
    "        visual_outputs_student = self.visual_student(images)\n",
    "        text_outputs_student = encode_text(self.text_student, texts)\n",
    "\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            visual_outputs_teacher = self.visual_teacher(images)\n",
    "            text_outputs_teacher = model.encode_text(texts)\n",
    "\n",
    "        # assert size\n",
    "        assert visual_outputs_student.size() == visual_outputs_teacher.size()\n",
    "        assert text_outputs_student.size() == text_outputs_teacher.size()\n",
    "\n",
    "        # KL Divergence Loss\n",
    "        kl_loss = KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "\n",
    "        text_kl_loss = kl_loss(\n",
    "            F.log_softmax(text_outputs_student), F.log_softmax(text_outputs_teacher)\n",
    "        )\n",
    "\n",
    "        image_kl_loss = kl_loss(\n",
    "            F.log_softmax(visual_outputs_student), F.log_softmax(visual_outputs_teacher)\n",
    "        )\n",
    "\n",
    "        # Push visual_outputs_student and text_outputs_student closer\n",
    "\n",
    "        image_embeddings = self.image_projection(visual_outputs_student)\n",
    "        text_embeddings = self.text_projection(text_outputs_student)\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "\n",
    "        texts_loss = self.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        images_loss = self.cross_entropy(logits.T, targets.T, reduction=\"none\")\n",
    "        con_loss = (images_loss + texts_loss) / 2.0\n",
    "        con_loss = con_loss.mean()\n",
    "\n",
    "        loss = (text_kl_loss + image_kl_loss + con_loss) / 3\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cross_entropy(self, preds, targets, reduction=\"none\"):\n",
    "        log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        loss = (-targets * log_softmax(preds)).sum(1)\n",
    "        if reduction == \"none\":\n",
    "            return loss\n",
    "        elif reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            loss_value = self._train_epoch(epoch)\n",
    "            print(f\"KLD-CosineLoss after {epoch} Epoch is {loss_value}\")\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        loss_value = 0\n",
    "        for batch_idx, data in enumerate(self.train_dataloader):\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            texts = data[\"caption\"]\n",
    "            images = data[\"image\"]\n",
    "\n",
    "            loss = self.compute_loss(images, texts)\n",
    "\n",
    "            loss_value += loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Loss after {batch_idx} Batch is {loss_value/(batch_idx+1)} \")\n",
    "\n",
    "        return loss_value.detach().cpu().numpy() / len(self.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63c467ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = DistillationTrainer(\n",
    "    visual_teacher_model=visual_teacher_model,\n",
    "    text_teacher_model=text_teacher_model,\n",
    "    text_student_model=text_student_model,\n",
    "    visual_student_model=visual_student_model,\n",
    "    train_dataloader=train_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c83373ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/ipykernel_launcher.py:61: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 Batch is 6.20703125 \n",
      "Loss after 100 Batch is 4.1015625 \n",
      "Loss after 200 Batch is 3.28515625 \n",
      "Loss after 300 Batch is 2.89453125 \n",
      "Loss after 400 Batch is 2.66015625 \n",
      "KLD-CosineLoss after 1 Epoch is 2.6013667425968108\n",
      "Loss after 0 Batch is 1.5693359375 \n",
      "Loss after 100 Batch is 1.7392578125 \n",
      "Loss after 200 Batch is 1.6806640625 \n",
      "Loss after 300 Batch is 1.6416015625 \n",
      "Loss after 400 Batch is 1.61328125 \n",
      "KLD-CosineLoss after 2 Epoch is 1.6059225512528474\n",
      "Loss after 0 Batch is 1.4443359375 \n",
      "Loss after 100 Batch is 1.4453125 \n",
      "Loss after 200 Batch is 1.427734375 \n",
      "Loss after 300 Batch is 1.408203125 \n",
      "Loss after 400 Batch is 1.400390625 \n",
      "KLD-CosineLoss after 3 Epoch is 1.4009111617312073\n",
      "Loss after 0 Batch is 1.3515625 \n",
      "Loss after 100 Batch is 1.3427734375 \n",
      "Loss after 200 Batch is 1.3232421875 \n",
      "Loss after 300 Batch is 1.3115234375 \n",
      "Loss after 400 Batch is 1.302734375 \n",
      "KLD-CosineLoss after 4 Epoch is 1.2972665148063782\n",
      "Loss after 0 Batch is 1.3349609375 \n",
      "Loss after 100 Batch is 1.240234375 \n",
      "Loss after 200 Batch is 1.2431640625 \n",
      "Loss after 300 Batch is 1.2431640625 \n",
      "Loss after 400 Batch is 1.2373046875 \n",
      "KLD-CosineLoss after 5 Epoch is 1.2323462414578588\n",
      "Loss after 0 Batch is 1.3349609375 \n",
      "Loss after 100 Batch is 1.193359375 \n",
      "Loss after 200 Batch is 1.1943359375 \n",
      "Loss after 300 Batch is 1.2001953125 \n",
      "Loss after 400 Batch is 1.2060546875 \n",
      "KLD-CosineLoss after 6 Epoch is 1.2015945330296127\n",
      "Loss after 0 Batch is 1.1181640625 \n",
      "Loss after 100 Batch is 1.1748046875 \n",
      "Loss after 200 Batch is 1.1630859375 \n",
      "Loss after 300 Batch is 1.1650390625 \n",
      "Loss after 400 Batch is 1.1591796875 \n",
      "KLD-CosineLoss after 7 Epoch is 1.1566059225512528\n",
      "Loss after 0 Batch is 1.091796875 \n",
      "Loss after 100 Batch is 1.140625 \n",
      "Loss after 200 Batch is 1.14453125 \n",
      "Loss after 300 Batch is 1.13671875 \n",
      "Loss after 400 Batch is 1.1337890625 \n",
      "KLD-CosineLoss after 8 Epoch is 1.1304100227790432\n",
      "Loss after 0 Batch is 1.0830078125 \n",
      "Loss after 100 Batch is 1.107421875 \n",
      "Loss after 200 Batch is 1.109375 \n",
      "Loss after 300 Batch is 1.1123046875 \n",
      "Loss after 400 Batch is 1.107421875 \n",
      "KLD-CosineLoss after 9 Epoch is 1.1047835990888382\n",
      "Loss after 0 Batch is 1.1025390625 \n",
      "Loss after 100 Batch is 1.0908203125 \n",
      "Loss after 200 Batch is 1.083984375 \n",
      "Loss after 300 Batch is 1.080078125 \n",
      "Loss after 400 Batch is 1.078125 \n",
      "KLD-CosineLoss after 10 Epoch is 1.076879271070615\n",
      "Loss after 0 Batch is 1.1240234375 \n",
      "Loss after 100 Batch is 1.072265625 \n",
      "Loss after 200 Batch is 1.0732421875 \n",
      "Loss after 300 Batch is 1.0751953125 \n",
      "Loss after 400 Batch is 1.0712890625 \n",
      "KLD-CosineLoss after 11 Epoch is 1.0700455580865604\n",
      "Loss after 0 Batch is 1.0146484375 \n",
      "Loss after 100 Batch is 1.0537109375 \n",
      "Loss after 200 Batch is 1.0546875 \n",
      "Loss after 300 Batch is 1.0537109375 \n",
      "Loss after 400 Batch is 1.048828125 \n",
      "KLD-CosineLoss after 12 Epoch is 1.046127562642369\n",
      "Loss after 0 Batch is 0.96240234375 \n",
      "Loss after 100 Batch is 1.0556640625 \n",
      "Loss after 200 Batch is 1.0546875 \n",
      "Loss after 300 Batch is 1.046875 \n",
      "Loss after 400 Batch is 1.0419921875 \n",
      "KLD-CosineLoss after 13 Epoch is 1.0404328018223234\n",
      "Loss after 0 Batch is 0.89794921875 \n",
      "Loss after 100 Batch is 1.0341796875 \n",
      "Loss after 200 Batch is 1.0302734375 \n",
      "Loss after 300 Batch is 1.03125 \n",
      "Loss after 400 Batch is 1.0322265625 \n",
      "KLD-CosineLoss after 14 Epoch is 1.0301822323462415\n",
      "Loss after 0 Batch is 1.11328125 \n",
      "Loss after 100 Batch is 1.0263671875 \n",
      "Loss after 200 Batch is 1.021484375 \n",
      "Loss after 300 Batch is 1.01953125 \n",
      "Loss after 400 Batch is 1.0185546875 \n",
      "KLD-CosineLoss after 15 Epoch is 1.0165148063781322\n",
      "Loss after 0 Batch is 1.0751953125 \n",
      "Loss after 100 Batch is 1.0009765625 \n",
      "Loss after 200 Batch is 1.0009765625 \n",
      "Loss after 300 Batch is 1.0068359375 \n",
      "Loss after 400 Batch is 1.0078125 \n",
      "KLD-CosineLoss after 16 Epoch is 1.0074031890660593\n",
      "Loss after 0 Batch is 1.076171875 \n",
      "Loss after 100 Batch is 1.0078125 \n",
      "Loss after 200 Batch is 1.005859375 \n",
      "Loss after 300 Batch is 0.9990234375 \n",
      "Loss after 400 Batch is 1.0 \n",
      "KLD-CosineLoss after 17 Epoch is 0.9988610478359908\n",
      "Loss after 0 Batch is 0.96435546875 \n",
      "Loss after 100 Batch is 0.9912109375 \n",
      "Loss after 200 Batch is 0.98876953125 \n",
      "Loss after 300 Batch is 0.99267578125 \n",
      "Loss after 400 Batch is 0.9951171875 \n",
      "KLD-CosineLoss after 18 Epoch is 0.9925968109339408\n",
      "Loss after 0 Batch is 0.93115234375 \n",
      "Loss after 100 Batch is 0.98779296875 \n",
      "Loss after 200 Batch is 0.97998046875 \n",
      "Loss after 300 Batch is 0.97998046875 \n",
      "Loss after 400 Batch is 0.984375 \n",
      "KLD-CosineLoss after 19 Epoch is 0.9851936218678815\n",
      "Loss after 0 Batch is 1.0615234375 \n",
      "Loss after 100 Batch is 0.9716796875 \n",
      "Loss after 200 Batch is 0.97021484375 \n",
      "Loss after 300 Batch is 0.97265625 \n",
      "Loss after 400 Batch is 0.970703125 \n",
      "KLD-CosineLoss after 20 Epoch is 0.9686788154897494\n",
      "Loss after 0 Batch is 0.873046875 \n",
      "Loss after 100 Batch is 0.96142578125 \n",
      "Loss after 200 Batch is 0.95458984375 \n",
      "Loss after 300 Batch is 0.95751953125 \n",
      "Loss after 400 Batch is 0.958984375 \n",
      "KLD-CosineLoss after 21 Epoch is 0.9607061503416856\n",
      "Loss after 0 Batch is 0.97998046875 \n",
      "Loss after 100 Batch is 0.962890625 \n",
      "Loss after 200 Batch is 0.95947265625 \n",
      "Loss after 300 Batch is 0.96240234375 \n",
      "Loss after 400 Batch is 0.9658203125 \n",
      "KLD-CosineLoss after 22 Epoch is 0.9624145785876993\n",
      "Loss after 0 Batch is 0.95068359375 \n",
      "Loss after 100 Batch is 0.94384765625 \n",
      "Loss after 200 Batch is 0.93603515625 \n",
      "Loss after 300 Batch is 0.943359375 \n",
      "Loss after 400 Batch is 0.94580078125 \n",
      "KLD-CosineLoss after 23 Epoch is 0.9453302961275627\n",
      "Loss after 0 Batch is 0.90185546875 \n",
      "Loss after 100 Batch is 0.94873046875 \n",
      "Loss after 200 Batch is 0.94384765625 \n",
      "Loss after 300 Batch is 0.943359375 \n",
      "Loss after 400 Batch is 0.9453125 \n",
      "KLD-CosineLoss after 24 Epoch is 0.943621867881549\n",
      "Loss after 0 Batch is 0.876953125 \n",
      "Loss after 100 Batch is 0.93115234375 \n",
      "Loss after 200 Batch is 0.927734375 \n",
      "Loss after 300 Batch is 0.93115234375 \n",
      "Loss after 400 Batch is 0.931640625 \n",
      "KLD-CosineLoss after 25 Epoch is 0.9316628701594533\n",
      "Loss after 0 Batch is 1.0205078125 \n",
      "Loss after 100 Batch is 0.92333984375 \n",
      "Loss after 200 Batch is 0.9287109375 \n",
      "Loss after 300 Batch is 0.9287109375 \n",
      "Loss after 400 Batch is 0.923828125 \n",
      "KLD-CosineLoss after 26 Epoch is 0.9214123006833713\n",
      "Loss after 0 Batch is 1.03515625 \n",
      "Loss after 100 Batch is 0.9228515625 \n",
      "Loss after 200 Batch is 0.92236328125 \n",
      "Loss after 300 Batch is 0.91259765625 \n",
      "Loss after 400 Batch is 0.912109375 \n",
      "KLD-CosineLoss after 27 Epoch is 0.9123006833712984\n",
      "Loss after 0 Batch is 0.86474609375 \n",
      "Loss after 100 Batch is 0.9091796875 \n",
      "Loss after 200 Batch is 0.91162109375 \n",
      "Loss after 300 Batch is 0.91796875 \n",
      "Loss after 400 Batch is 0.91357421875 \n",
      "KLD-CosineLoss after 28 Epoch is 0.9162870159453302\n",
      "Loss after 0 Batch is 0.94677734375 \n",
      "Loss after 100 Batch is 0.8916015625 \n",
      "Loss after 200 Batch is 0.88916015625 \n",
      "Loss after 300 Batch is 0.89453125 \n",
      "Loss after 400 Batch is 0.90576171875 \n",
      "KLD-CosineLoss after 29 Epoch is 0.9043280182232346\n",
      "Loss after 0 Batch is 0.79345703125 \n",
      "Loss after 100 Batch is 0.88427734375 \n",
      "Loss after 200 Batch is 0.88818359375 \n",
      "Loss after 300 Batch is 0.888671875 \n",
      "Loss after 400 Batch is 0.89208984375 \n",
      "KLD-CosineLoss after 30 Epoch is 0.8917995444191344\n"
     ]
    }
   ],
   "source": [
    "Trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a3c9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Trainer.visual_student.state_dict(), f\"CombinedVisual_DistilledModel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4e6ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Trainer.text_student.state_dict(), f\"CombinedText_DistilledModel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309f3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6b0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
