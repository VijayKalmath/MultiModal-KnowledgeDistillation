{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87273e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import KLDivLoss, CrossEntropyLoss, CosineEmbeddingLoss,MSELoss\n",
    "from torch.optim import Adam,SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd6d6",
   "metadata": {},
   "source": [
    "### Loading Teacher model ---> CLIP Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88cbfadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 37,828,608\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "model_name = \"ViT-B/32\"\n",
    "\n",
    "# model is the torch model.\n",
    "# preprocess function is for image preprocessing.\n",
    "\n",
    "model, preprocess = clip.load(model_name)\n",
    "\n",
    "# Get the transformer model\n",
    "teacher_model = model.transformer\n",
    "\n",
    "print(\n",
    "    \"Model parameters:\",\n",
    "    f\"{np.sum([int(np.prod(p.shape)) for p in model.transformer.parameters()]):,}\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896653c",
   "metadata": {},
   "source": [
    "### Instantiating Student model \n",
    "\n",
    "[Transformer](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/model.py#L195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a1340820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 18,914,304\n"
     ]
    }
   ],
   "source": [
    "from clip.model import Transformer\n",
    "from clip.model import convert_weights # Make them float16\n",
    "# Set Student Configuration\n",
    "\n",
    "width = 512\n",
    "layers = 6\n",
    "heads = 8 # More Number of Heads \n",
    "\n",
    "def build_attention_mask():\n",
    "    context_length = 77 \n",
    "    mask = torch.empty(context_length,context_length)\n",
    "    mask.fill_(float(\"-inf\"))\n",
    "    mask.triu_(1)  # zero out the lower diagonal\n",
    "    return mask\n",
    "    \n",
    "student_model = Transformer(\n",
    "    width=width,\n",
    "    layers=layers,\n",
    "    heads=heads,\n",
    "     attn_mask=build_attention_mask()\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "convert_weights(student_model)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Model parameters:\",\n",
    "    f\"{np.sum([int(np.prod(p.shape)) for p in student_model.parameters()]):,}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8900fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(transformer, text):\n",
    "    \n",
    "    x = model.token_embedding(text).type(model.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "    x = x + model.positional_embedding.type(model.dtype)\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "    \n",
    "    x = transformer(x)\n",
    "    \n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "    x = model.ln_final(x).type(model.dtype)\n",
    "\n",
    "    # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "    # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "    x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ model.text_projection\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6debb0c",
   "metadata": {},
   "source": [
    "### Load the WIT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6383d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: conceptual_captions/unlabeled\n",
      "Reusing dataset conceptual_captions (/home/ecbm4040/.cache/huggingface/datasets/conceptual_captions/unlabeled/1.0.0/05266784888422e36944016874c44639bccb39069c2227435168ad8b02d600d8)\n",
      "Loading cached processed dataset at /home/ecbm4040/.cache/huggingface/datasets/conceptual_captions/unlabeled/1.0.0/05266784888422e36944016874c44639bccb39069c2227435168ad8b02d600d8/cache-d49e7748d48fd605.arrow\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import io\n",
    "import urllib\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "\n",
    "num_threads = 20\n",
    "dset = load_dataset(\"conceptual_captions\",split='train[:10000]')\n",
    "\n",
    "dset = dset.remove_columns(\"image_url\")\n",
    "\n",
    "dset = dset.filter(lambda example: len(example[\"caption\"]) < 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a6b1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dset, batch_size=16,shuffle=True,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "609fd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDistillationTrainer:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.train_dataloader = train_dataloader\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.teacher = self.teacher.to(self.device)\n",
    "        self.student = self.student.to(self.device)\n",
    "        self.teacher.eval()\n",
    "\n",
    "        self.epochs = 30\n",
    "        self.start_epoch = 1\n",
    "\n",
    "        # set up optimizer\n",
    "        self.optimizer = SGD(self.student.parameters(),lr = 0.001)\n",
    "\n",
    "        \n",
    "    def compute_loss(self, texts, return_outputs=False):\n",
    "        texts = clip.tokenize(texts)\n",
    "        \n",
    "        texts = texts.to(self.device)\n",
    "\n",
    "        outputs_student = encode_text(self.student,texts)\n",
    "        \n",
    "        # compute teacher output\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = model.encode_text(texts)\n",
    "#             outputs_teacher = torch.tensor(outputs_teacher.detach().cpu().numpy()).to(self.device)\n",
    "        \n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.size() == outputs_teacher.size()\n",
    "\n",
    "        # KL Divergence Loss        \n",
    "        kl_loss = KLDivLoss(reduction=\"batchmean\",log_target =True)\n",
    "        loss = kl_loss(F.log_softmax(outputs_student),F.log_softmax(outputs_teacher))\n",
    "\n",
    "        # Cosine loss\n",
    "        loss += CosineEmbeddingLoss()(\n",
    "            outputs_teacher, outputs_student, torch.ones(outputs_teacher.size()[0]).to(self.device)\n",
    "        )\n",
    "        \n",
    "#         #MSE Loss \n",
    "#         mse_loss = MSELoss()\n",
    "#         loss += mse_loss(outputs_teacher, outputs_student)\n",
    "\n",
    "\n",
    "        return  loss\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            loss_value = self._train_epoch(epoch)\n",
    "            print(f\"KLD-CosineLoss after {epoch} Epoch is {loss_value}\")\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        loss_value = 0\n",
    "        for batch_idx, data in enumerate(self.train_dataloader):\n",
    "        \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            texts = data[\"caption\"]\n",
    "\n",
    "            loss = self.compute_loss(texts)\n",
    "            \n",
    "            loss_value += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Loss after {batch_idx} Batch is {loss_value/(batch_idx+1)} \")\n",
    "\n",
    "        return loss_value.detach().cpu().numpy() / len(self.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "63c467ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = TextDistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_dataloader=train_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c83373ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 Batch is 2.30859375 \n",
      "Loss after 100 Batch is 0.6328125 \n",
      "Loss after 200 Batch is 0.595703125 \n",
      "Loss after 300 Batch is 0.5751953125 \n",
      "Loss after 400 Batch is 0.5634765625 \n",
      "Loss after 500 Batch is 0.5537109375 \n",
      "KLD-CosineLoss after 1 Epoch is 0.5530451866404715\n",
      "Loss after 0 Batch is 0.453369140625 \n",
      "Loss after 100 Batch is 0.4990234375 \n",
      "Loss after 200 Batch is 0.4951171875 \n",
      "Loss after 300 Batch is 0.490966796875 \n",
      "Loss after 400 Batch is 0.489501953125 \n",
      "Loss after 500 Batch is 0.486083984375 \n",
      "KLD-CosineLoss after 2 Epoch is 0.4862475442043222\n",
      "Loss after 0 Batch is 0.47802734375 \n",
      "Loss after 100 Batch is 0.46533203125 \n",
      "Loss after 200 Batch is 0.46044921875 \n",
      "Loss after 300 Batch is 0.460205078125 \n",
      "Loss after 400 Batch is 0.456298828125 \n",
      "Loss after 500 Batch is 0.453369140625 \n",
      "KLD-CosineLoss after 3 Epoch is 0.4535854616895874\n",
      "Loss after 0 Batch is 0.45654296875 \n",
      "Loss after 100 Batch is 0.431396484375 \n",
      "Loss after 200 Batch is 0.429443359375 \n",
      "Loss after 300 Batch is 0.432373046875 \n",
      "Loss after 400 Batch is 0.43017578125 \n",
      "Loss after 500 Batch is 0.429443359375 \n",
      "KLD-CosineLoss after 4 Epoch is 0.4292730844793713\n",
      "Loss after 0 Batch is 0.3955078125 \n",
      "Loss after 100 Batch is 0.4228515625 \n",
      "Loss after 200 Batch is 0.419189453125 \n",
      "Loss after 300 Batch is 0.415771484375 \n",
      "Loss after 400 Batch is 0.411865234375 \n",
      "Loss after 500 Batch is 0.409912109375 \n",
      "KLD-CosineLoss after 5 Epoch is 0.40962671905697445\n",
      "Loss after 0 Batch is 0.406005859375 \n",
      "Loss after 100 Batch is 0.403076171875 \n",
      "Loss after 200 Batch is 0.3974609375 \n",
      "Loss after 300 Batch is 0.398681640625 \n",
      "Loss after 400 Batch is 0.397705078125 \n",
      "Loss after 500 Batch is 0.3974609375 \n",
      "KLD-CosineLoss after 6 Epoch is 0.3973477406679764\n",
      "Loss after 0 Batch is 0.44189453125 \n",
      "Loss after 100 Batch is 0.3935546875 \n",
      "Loss after 200 Batch is 0.389892578125 \n",
      "Loss after 300 Batch is 0.3896484375 \n",
      "Loss after 400 Batch is 0.38818359375 \n",
      "Loss after 500 Batch is 0.38720703125 \n",
      "KLD-CosineLoss after 7 Epoch is 0.38727897838899805\n",
      "Loss after 0 Batch is 0.32275390625 \n",
      "Loss after 100 Batch is 0.381591796875 \n",
      "Loss after 200 Batch is 0.37841796875 \n",
      "Loss after 300 Batch is 0.37890625 \n",
      "Loss after 400 Batch is 0.379150390625 \n",
      "Loss after 500 Batch is 0.37744140625 \n",
      "KLD-CosineLoss after 8 Epoch is 0.37770137524557956\n",
      "Loss after 0 Batch is 0.3818359375 \n",
      "Loss after 100 Batch is 0.37158203125 \n",
      "Loss after 200 Batch is 0.37158203125 \n",
      "Loss after 300 Batch is 0.369873046875 \n",
      "Loss after 400 Batch is 0.369384765625 \n",
      "Loss after 500 Batch is 0.368896484375 \n",
      "KLD-CosineLoss after 9 Epoch is 0.36910609037328096\n",
      "Loss after 0 Batch is 0.33056640625 \n",
      "Loss after 100 Batch is 0.36181640625 \n",
      "Loss after 200 Batch is 0.361328125 \n",
      "Loss after 300 Batch is 0.3603515625 \n",
      "Loss after 400 Batch is 0.3603515625 \n",
      "Loss after 500 Batch is 0.36181640625 \n",
      "KLD-CosineLoss after 10 Epoch is 0.3619842829076621\n",
      "Loss after 0 Batch is 0.4130859375 \n",
      "Loss after 100 Batch is 0.352294921875 \n",
      "Loss after 200 Batch is 0.35498046875 \n",
      "Loss after 300 Batch is 0.353759765625 \n",
      "Loss after 400 Batch is 0.35595703125 \n",
      "Loss after 500 Batch is 0.357421875 \n",
      "KLD-CosineLoss after 11 Epoch is 0.3575638506876228\n",
      "Loss after 0 Batch is 0.37890625 \n",
      "Loss after 100 Batch is 0.34765625 \n",
      "Loss after 200 Batch is 0.3515625 \n",
      "Loss after 300 Batch is 0.354736328125 \n",
      "Loss after 400 Batch is 0.353271484375 \n",
      "Loss after 500 Batch is 0.3525390625 \n",
      "KLD-CosineLoss after 12 Epoch is 0.3526522593320236\n",
      "Loss after 0 Batch is 0.36572265625 \n",
      "Loss after 100 Batch is 0.3369140625 \n",
      "Loss after 200 Batch is 0.339599609375 \n",
      "Loss after 300 Batch is 0.34375 \n",
      "Loss after 400 Batch is 0.3466796875 \n",
      "Loss after 500 Batch is 0.34814453125 \n",
      "KLD-CosineLoss after 13 Epoch is 0.34847740667976423\n",
      "Loss after 0 Batch is 0.31640625 \n",
      "Loss after 100 Batch is 0.34228515625 \n",
      "Loss after 200 Batch is 0.34326171875 \n",
      "Loss after 300 Batch is 0.34375 \n",
      "Loss after 400 Batch is 0.34326171875 \n",
      "Loss after 500 Batch is 0.343017578125 \n",
      "KLD-CosineLoss after 14 Epoch is 0.3430746561886051\n",
      "Loss after 0 Batch is 0.35693359375 \n",
      "Loss after 100 Batch is 0.340576171875 \n",
      "Loss after 200 Batch is 0.339599609375 \n",
      "Loss after 300 Batch is 0.339599609375 \n",
      "Loss after 400 Batch is 0.337890625 \n",
      "Loss after 500 Batch is 0.338623046875 \n",
      "KLD-CosineLoss after 15 Epoch is 0.33840864440078583\n",
      "Loss after 0 Batch is 0.35986328125 \n",
      "Loss after 100 Batch is 0.335693359375 \n",
      "Loss after 200 Batch is 0.33544921875 \n",
      "Loss after 300 Batch is 0.33642578125 \n",
      "Loss after 400 Batch is 0.33740234375 \n",
      "Loss after 500 Batch is 0.337158203125 \n",
      "KLD-CosineLoss after 16 Epoch is 0.3369351669941061\n",
      "Loss after 0 Batch is 0.32177734375 \n",
      "Loss after 100 Batch is 0.330078125 \n",
      "Loss after 200 Batch is 0.329345703125 \n",
      "Loss after 300 Batch is 0.3310546875 \n",
      "Loss after 400 Batch is 0.3330078125 \n",
      "Loss after 500 Batch is 0.33349609375 \n",
      "KLD-CosineLoss after 17 Epoch is 0.3334970530451866\n",
      "Loss after 0 Batch is 0.320556640625 \n",
      "Loss after 100 Batch is 0.32958984375 \n",
      "Loss after 200 Batch is 0.3291015625 \n",
      "Loss after 300 Batch is 0.331298828125 \n",
      "Loss after 400 Batch is 0.33203125 \n",
      "Loss after 500 Batch is 0.332275390625 \n",
      "KLD-CosineLoss after 18 Epoch is 0.3327603143418468\n",
      "Loss after 0 Batch is 0.306884765625 \n",
      "Loss after 100 Batch is 0.331298828125 \n",
      "Loss after 200 Batch is 0.329833984375 \n",
      "Loss after 300 Batch is 0.328369140625 \n",
      "Loss after 400 Batch is 0.328857421875 \n",
      "Loss after 500 Batch is 0.33056640625 \n",
      "KLD-CosineLoss after 19 Epoch is 0.33104125736738704\n",
      "Loss after 0 Batch is 0.377685546875 \n",
      "Loss after 100 Batch is 0.322998046875 \n",
      "Loss after 200 Batch is 0.328125 \n",
      "Loss after 300 Batch is 0.328125 \n",
      "Loss after 400 Batch is 0.327880859375 \n",
      "Loss after 500 Batch is 0.330078125 \n",
      "KLD-CosineLoss after 20 Epoch is 0.3300589390962672\n",
      "Loss after 0 Batch is 0.31201171875 \n",
      "Loss after 100 Batch is 0.32421875 \n",
      "Loss after 200 Batch is 0.3251953125 \n",
      "Loss after 300 Batch is 0.324462890625 \n",
      "Loss after 400 Batch is 0.3251953125 \n",
      "Loss after 500 Batch is 0.328369140625 \n",
      "KLD-CosineLoss after 21 Epoch is 0.3280943025540275\n",
      "Loss after 0 Batch is 0.307373046875 \n",
      "Loss after 100 Batch is 0.3251953125 \n",
      "Loss after 200 Batch is 0.32421875 \n",
      "Loss after 300 Batch is 0.32275390625 \n",
      "Loss after 400 Batch is 0.322998046875 \n",
      "Loss after 500 Batch is 0.32470703125 \n",
      "KLD-CosineLoss after 22 Epoch is 0.3241650294695481\n",
      "Loss after 0 Batch is 0.31640625 \n",
      "Loss after 100 Batch is 0.322021484375 \n",
      "Loss after 200 Batch is 0.324951171875 \n",
      "Loss after 300 Batch is 0.32373046875 \n",
      "Loss after 400 Batch is 0.322265625 \n",
      "Loss after 500 Batch is 0.322998046875 \n",
      "KLD-CosineLoss after 23 Epoch is 0.32293713163064836\n",
      "Loss after 0 Batch is 0.24755859375 \n",
      "Loss after 100 Batch is 0.319580078125 \n",
      "Loss after 200 Batch is 0.320556640625 \n",
      "Loss after 300 Batch is 0.322265625 \n",
      "Loss after 400 Batch is 0.322265625 \n",
      "Loss after 500 Batch is 0.320556640625 \n",
      "KLD-CosineLoss after 24 Epoch is 0.3204813359528487\n",
      "Loss after 0 Batch is 0.3193359375 \n",
      "Loss after 100 Batch is 0.31982421875 \n",
      "Loss after 200 Batch is 0.320556640625 \n",
      "Loss after 300 Batch is 0.319580078125 \n",
      "Loss after 400 Batch is 0.319091796875 \n",
      "Loss after 500 Batch is 0.31982421875 \n",
      "KLD-CosineLoss after 25 Epoch is 0.32023575638506874\n",
      "Loss after 0 Batch is 0.3359375 \n",
      "Loss after 100 Batch is 0.314697265625 \n",
      "Loss after 200 Batch is 0.316650390625 \n",
      "Loss after 300 Batch is 0.317626953125 \n",
      "Loss after 400 Batch is 0.31787109375 \n",
      "Loss after 500 Batch is 0.318359375 \n",
      "KLD-CosineLoss after 26 Epoch is 0.3182711198428291\n",
      "Loss after 0 Batch is 0.335205078125 \n",
      "Loss after 100 Batch is 0.313232421875 \n",
      "Loss after 200 Batch is 0.312255859375 \n",
      "Loss after 300 Batch is 0.31591796875 \n",
      "Loss after 400 Batch is 0.317138671875 \n",
      "Loss after 500 Batch is 0.31884765625 \n",
      "KLD-CosineLoss after 27 Epoch is 0.31851669941060906\n",
      "Loss after 0 Batch is 0.337158203125 \n",
      "Loss after 100 Batch is 0.318115234375 \n",
      "Loss after 200 Batch is 0.31787109375 \n",
      "Loss after 300 Batch is 0.3154296875 \n",
      "Loss after 400 Batch is 0.314697265625 \n",
      "Loss after 500 Batch is 0.317626953125 \n",
      "KLD-CosineLoss after 28 Epoch is 0.3175343811394892\n",
      "Loss after 0 Batch is 0.36865234375 \n",
      "Loss after 100 Batch is 0.31689453125 \n",
      "Loss after 200 Batch is 0.31591796875 \n",
      "Loss after 300 Batch is 0.315185546875 \n",
      "Loss after 400 Batch is 0.316162109375 \n",
      "Loss after 500 Batch is 0.315673828125 \n",
      "KLD-CosineLoss after 29 Epoch is 0.31581532416502944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 Batch is 0.342529296875 \n",
      "Loss after 100 Batch is 0.318115234375 \n",
      "Loss after 200 Batch is 0.31396484375 \n",
      "Loss after 300 Batch is 0.3134765625 \n",
      "Loss after 400 Batch is 0.31396484375 \n",
      "Loss after 500 Batch is 0.315185546875 \n",
      "KLD-CosineLoss after 30 Epoch is 0.3153241650294695\n"
     ]
    }
   ],
   "source": [
    "Trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b767c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Trainer.student.state_dict(),f\"Text_DistilledModel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068534a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
