{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87273e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import KLDivLoss, CrossEntropyLoss, CosineEmbeddingLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd6d6",
   "metadata": {},
   "source": [
    "### Loading Teacher model ---> CLIP Image Extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cbfadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 87,849,216\n",
      "Input resolution: 224\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "model_name = \"ViT-B/32\"\n",
    "\n",
    "# model is the torch model.\n",
    "# preprocess function is for image preprocessing.\n",
    "\n",
    "model, preprocess = clip.load(model_name)\n",
    "\n",
    "# Get only the visual model\n",
    "teacher_model = model.visual\n",
    "input_resolution = model.visual.input_resolution\n",
    "\n",
    "print(\n",
    "    \"Model parameters:\",\n",
    "    f\"{np.sum([int(np.prod(p.shape)) for p in model.visual.parameters()]):,}\",\n",
    ")\n",
    "print(\"Input resolution:\", input_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7facf597",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (resblocks): Sequential(\n",
       "    (0): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): ResidualAttentionBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (gelu): QuickGELU()\n",
       "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896653c",
   "metadata": {},
   "source": [
    "### Instantiating Student model \n",
    "\n",
    "[VisionTransformer](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/model.py#L206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1340820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 12,044,160\n"
     ]
    }
   ],
   "source": [
    "from clip.model import VisionTransformer\n",
    "from clip.model import convert_weights  # Make them float16\n",
    "\n",
    "# Set Student Configuration\n",
    "\n",
    "patch_size = 32\n",
    "width = 384\n",
    "layers = 6\n",
    "heads = 12\n",
    "output_dim = 512\n",
    "\n",
    "student_model = VisionTransformer(\n",
    "    input_resolution=input_resolution,\n",
    "    patch_size=patch_size,\n",
    "    width=width,\n",
    "    layers=layers,\n",
    "    heads=heads,\n",
    "    output_dim=output_dim,\n",
    ")\n",
    "\n",
    "\n",
    "convert_weights(student_model)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Model parameters:\",\n",
    "    f\"{np.sum([int(np.prod(p.shape)) for p in student_model.parameters()]):,}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6debb0c",
   "metadata": {},
   "source": [
    "### Load the WIT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6383d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: conceptual_captions/unlabeled\n",
      "Reusing dataset conceptual_captions (/home/ecbm4040/.cache/huggingface/datasets/conceptual_captions/unlabeled/1.0.0/05266784888422e36944016874c44639bccb39069c2227435168ad8b02d600d8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8e81a4437f45998654cb1f7a7a0675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688e80d40fb94c65ada7f46ed749744e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/829584 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import io\n",
    "import urllib\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "\n",
    "\n",
    "def fetch_single_image(image_url, timeout=None, retries=0):\n",
    "    for _ in range(retries + 1):\n",
    "        try:\n",
    "            request = urllib.request.Request(\n",
    "                image_url,\n",
    "                data=None,\n",
    "                headers={\"user-agent\": get_datasets_user_agent()},\n",
    "            )\n",
    "            with urllib.request.urlopen(request, timeout=timeout) as req:\n",
    "                image = PIL.Image.open(io.BytesIO(req.read()))\n",
    "            break\n",
    "        except Exception:\n",
    "            image = None\n",
    "    return image\n",
    "\n",
    "\n",
    "def fetch_images(batch, num_threads, timeout=None, retries=0):\n",
    "    fetch_single_image_with_args = partial(\n",
    "        fetch_single_image, timeout=timeout, retries=retries\n",
    "    )\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        batch[\"image\"] = list(\n",
    "            executor.map(fetch_single_image_with_args, batch[\"image_url\"])\n",
    "        )\n",
    "    return batch\n",
    "\n",
    "\n",
    "num_threads = 1\n",
    "# dset = load_dataset(\"cifar10\")\n",
    "dset = load_dataset(\"conceptual_captions\")\n",
    "dset = dset.map(\n",
    "    fetch_images, batched=True, batch_size=4, fn_kwargs={\"num_threads\": num_threads}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a1997fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0, 0, 0), (1, 1, 1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cifar100 = torchvision.datasets.CIFAR100(\n",
    "    \"data/\", download=True, train=True, transform=transform\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    cifar100, batch_size=32, shuffle=True, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "609fd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.train_dataloader = train_dataloader\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.teacher = self.teacher.to(self.device)\n",
    "        self.student = self.student.to(self.device)\n",
    "        self.teacher.eval()\n",
    "\n",
    "        self.epochs = 30\n",
    "        self.start_epoch = 1\n",
    "\n",
    "        # set up optimizer\n",
    "        self.optimizer = SGD(self.student.parameters(), lr=0.001)\n",
    "\n",
    "        # Set up LR Scheduler\n",
    "\n",
    "    #         self.lr_scheduler = ReduceLROnPlateau(self.optimizer, \"min\")\n",
    "\n",
    "    def compute_loss(self, images, return_outputs=False):\n",
    "        images = images.to(self.device).half()\n",
    "\n",
    "        outputs_student = self.student(images)\n",
    "\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(images)\n",
    "        #             outputs_teacher = torch.tensor(outputs_teacher.detach().cpu().numpy()).to(self.device)\n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.size() == outputs_teacher.size()\n",
    "\n",
    "        # Soften probabilities and compute distillation loss\n",
    "\n",
    "        #         KL Divergence Loss\n",
    "        kl_loss = KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "        loss = kl_loss(F.log_softmax(outputs_student), F.log_softmax(outputs_teacher))\n",
    "\n",
    "        # Cosine loss\n",
    "        loss += CosineEmbeddingLoss()(\n",
    "            outputs_teacher,\n",
    "            outputs_student,\n",
    "            torch.ones(outputs_teacher.size()[0]).to(self.device),\n",
    "        )\n",
    "\n",
    "        #         #MSE Loss\n",
    "        #         mse_loss = MSELoss()\n",
    "        #         loss += mse_loss(outputs_teacher, outputs_student)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            loss_value = self._train_epoch(epoch)\n",
    "            print(f\"KLD-CosineLoss after {epoch} Epoch is {loss_value}\")\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        loss_value = 0\n",
    "        for batch_idx, (images, _) in enumerate(self.train_dataloader):\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            #             labels = torch.nn.functional.one_hot(labels,num_classes=100)\n",
    "\n",
    "            loss = self.compute_loss(images)\n",
    "\n",
    "            loss_value += loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            #             torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            #             print(\"After\",self.student.conv1.weight)\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Loss after {batch_idx} Batch is {loss_value/(batch_idx+1)} \")\n",
    "\n",
    "        return loss_value.detach().cpu().numpy() / len(self.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "63c467ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = DistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    preprocess=preprocess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83373ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 Batch is 1.5751953125 \n",
      "Loss after 10 Batch is 1.4755859375 \n",
      "Loss after 20 Batch is 1.38671875 \n",
      "Loss after 30 Batch is 1.3125 \n",
      "Loss after 40 Batch is 1.2490234375 \n",
      "Loss after 50 Batch is 1.193359375 \n",
      "Loss after 60 Batch is 1.1455078125 \n",
      "Loss after 70 Batch is 1.1044921875 \n",
      "Loss after 80 Batch is 1.068359375 \n",
      "Loss after 90 Batch is 1.033203125 \n",
      "Loss after 100 Batch is 1.001953125 \n",
      "Loss after 110 Batch is 0.9736328125 \n",
      "Loss after 120 Batch is 0.9462890625 \n",
      "Loss after 130 Batch is 0.921875 \n",
      "Loss after 140 Batch is 0.89990234375 \n",
      "Loss after 150 Batch is 0.880859375 \n",
      "Loss after 160 Batch is 0.86083984375 \n",
      "Loss after 170 Batch is 0.83984375 \n",
      "Loss after 180 Batch is 0.8212890625 \n",
      "Loss after 190 Batch is 0.80419921875 \n",
      "Loss after 200 Batch is 0.7890625 \n",
      "Loss after 210 Batch is 0.775390625 \n",
      "Loss after 220 Batch is 0.76318359375 \n",
      "Loss after 230 Batch is 0.75146484375 \n",
      "Loss after 240 Batch is 0.7412109375 \n",
      "Loss after 250 Batch is 0.72900390625 \n",
      "Loss after 260 Batch is 0.71630859375 \n",
      "Loss after 270 Batch is 0.7041015625 \n",
      "Loss after 280 Batch is 0.6923828125 \n",
      "Loss after 290 Batch is 0.68115234375 \n",
      "Loss after 300 Batch is 0.6708984375 \n",
      "Loss after 310 Batch is 0.66162109375 \n",
      "Loss after 320 Batch is 0.65283203125 \n",
      "Loss after 330 Batch is 0.64404296875 \n",
      "Loss after 340 Batch is 0.63623046875 \n",
      "Loss after 350 Batch is 0.62890625 \n",
      "Loss after 360 Batch is 0.6220703125 \n",
      "Loss after 370 Batch is 0.615234375 \n",
      "Loss after 380 Batch is 0.60888671875 \n",
      "Loss after 390 Batch is 0.60302734375 \n",
      "Loss after 400 Batch is 0.59716796875 \n",
      "Loss after 410 Batch is 0.591796875 \n",
      "Loss after 420 Batch is 0.5869140625 \n",
      "Loss after 430 Batch is 0.58154296875 \n",
      "Loss after 440 Batch is 0.57666015625 \n",
      "Loss after 450 Batch is 0.5703125 \n",
      "Loss after 460 Batch is 0.5634765625 \n",
      "Loss after 470 Batch is 0.556640625 \n",
      "Loss after 480 Batch is 0.55029296875 \n",
      "Loss after 490 Batch is 0.54443359375 \n",
      "Loss after 500 Batch is 0.53857421875 \n",
      "Loss after 510 Batch is 0.53271484375 \n",
      "Loss after 520 Batch is 0.52734375 \n",
      "Loss after 530 Batch is 0.52197265625 \n",
      "Loss after 540 Batch is 0.51708984375 \n",
      "Loss after 550 Batch is 0.51220703125 \n",
      "Loss after 560 Batch is 0.5078125 \n",
      "Loss after 570 Batch is 0.5029296875 \n",
      "Loss after 580 Batch is 0.498779296875 \n",
      "Loss after 590 Batch is 0.494384765625 \n",
      "Loss after 600 Batch is 0.490478515625 \n",
      "Loss after 610 Batch is 0.486572265625 \n",
      "Loss after 620 Batch is 0.482666015625 \n",
      "Loss after 630 Batch is 0.47900390625 \n",
      "Loss after 640 Batch is 0.475341796875 \n",
      "Loss after 650 Batch is 0.471923828125 \n",
      "Loss after 660 Batch is 0.468505859375 \n",
      "Loss after 670 Batch is 0.46533203125 \n",
      "Loss after 680 Batch is 0.462158203125 \n",
      "Loss after 690 Batch is 0.459228515625 \n",
      "Loss after 700 Batch is 0.4560546875 \n",
      "Loss after 710 Batch is 0.453125 \n",
      "Loss after 720 Batch is 0.450439453125 \n",
      "Loss after 730 Batch is 0.44775390625 \n",
      "Loss after 740 Batch is 0.445068359375 \n",
      "Loss after 750 Batch is 0.4423828125 \n",
      "Loss after 760 Batch is 0.43994140625 \n",
      "Loss after 770 Batch is 0.4375 \n",
      "Loss after 780 Batch is 0.43505859375 \n",
      "Loss after 790 Batch is 0.4326171875 \n",
      "Loss after 800 Batch is 0.430419921875 \n",
      "Loss after 810 Batch is 0.42822265625 \n",
      "Loss after 820 Batch is 0.426025390625 \n",
      "Loss after 830 Batch is 0.423828125 \n",
      "Loss after 840 Batch is 0.421875 \n",
      "Loss after 850 Batch is 0.419921875 \n",
      "Loss after 860 Batch is 0.417724609375 \n",
      "Loss after 870 Batch is 0.416015625 \n",
      "Loss after 880 Batch is 0.4140625 \n",
      "Loss after 890 Batch is 0.412109375 \n",
      "Loss after 900 Batch is 0.410400390625 \n",
      "Loss after 910 Batch is 0.40869140625 \n",
      "Loss after 920 Batch is 0.406982421875 \n",
      "Loss after 930 Batch is 0.4052734375 \n",
      "Loss after 940 Batch is 0.403564453125 \n",
      "Loss after 950 Batch is 0.40185546875 \n",
      "Loss after 960 Batch is 0.400390625 \n",
      "Loss after 970 Batch is 0.39892578125 \n",
      "Loss after 980 Batch is 0.397216796875 \n",
      "Loss after 990 Batch is 0.395751953125 \n",
      "Loss after 1000 Batch is 0.394287109375 \n",
      "Loss after 1010 Batch is 0.392822265625 \n",
      "Loss after 1020 Batch is 0.3916015625 \n",
      "Loss after 1030 Batch is 0.39013671875 \n",
      "Loss after 1040 Batch is 0.388916015625 \n",
      "Loss after 1050 Batch is 0.387451171875 \n",
      "Loss after 1060 Batch is 0.38623046875 \n",
      "Loss after 1070 Batch is 0.385009765625 \n",
      "Loss after 1080 Batch is 0.3837890625 \n",
      "Loss after 1090 Batch is 0.382568359375 \n",
      "Loss after 1100 Batch is 0.38134765625 \n",
      "Loss after 1110 Batch is 0.380126953125 \n",
      "Loss after 1120 Batch is 0.37890625 \n",
      "Loss after 1130 Batch is 0.377685546875 \n",
      "Loss after 1140 Batch is 0.376708984375 \n",
      "Loss after 1150 Batch is 0.37548828125 \n",
      "Loss after 1160 Batch is 0.37451171875 \n",
      "Loss after 1170 Batch is 0.373291015625 \n",
      "Loss after 1180 Batch is 0.372314453125 \n",
      "Loss after 1190 Batch is 0.371337890625 \n",
      "Loss after 1200 Batch is 0.370361328125 \n",
      "Loss after 1210 Batch is 0.369384765625 \n",
      "Loss after 1220 Batch is 0.368408203125 \n",
      "Loss after 1230 Batch is 0.367431640625 \n",
      "Loss after 1240 Batch is 0.366455078125 \n",
      "Loss after 1250 Batch is 0.365478515625 \n",
      "Loss after 1260 Batch is 0.364501953125 \n",
      "Loss after 1270 Batch is 0.36376953125 \n",
      "Loss after 1280 Batch is 0.36279296875 \n",
      "Loss after 1290 Batch is 0.36181640625 \n",
      "Loss after 1300 Batch is 0.361083984375 \n",
      "Loss after 1310 Batch is 0.360107421875 \n",
      "Loss after 1320 Batch is 0.359375 \n",
      "Loss after 1330 Batch is 0.358642578125 \n",
      "Loss after 1340 Batch is 0.357666015625 \n",
      "Loss after 1350 Batch is 0.35693359375 \n",
      "Loss after 1360 Batch is 0.356201171875 \n",
      "Loss after 1370 Batch is 0.35546875 \n",
      "Loss after 1380 Batch is 0.354736328125 \n",
      "Loss after 1390 Batch is 0.35400390625 \n",
      "Loss after 1400 Batch is 0.35302734375 \n",
      "Loss after 1410 Batch is 0.352294921875 \n",
      "Loss after 1420 Batch is 0.351806640625 \n",
      "Loss after 1430 Batch is 0.35107421875 \n",
      "Loss after 1440 Batch is 0.350341796875 \n",
      "Loss after 1450 Batch is 0.349609375 \n",
      "Loss after 1460 Batch is 0.348876953125 \n",
      "Loss after 1470 Batch is 0.34814453125 \n",
      "Loss after 1480 Batch is 0.345703125 \n",
      "Loss after 1490 Batch is 0.343505859375 \n",
      "Loss after 1500 Batch is 0.341064453125 \n",
      "Loss after 1510 Batch is 0.3388671875 \n",
      "Loss after 1520 Batch is 0.336669921875 \n",
      "Loss after 1530 Batch is 0.33447265625 \n",
      "Loss after 1540 Batch is 0.332275390625 \n",
      "Loss after 1550 Batch is 0.330078125 \n",
      "Loss after 1560 Batch is 0.327880859375 \n",
      "KLD-CosineLoss after 1 Epoch is 0.327575175943698\n",
      "Loss after 0 Batch is 0.1602783203125 \n",
      "Loss after 10 Batch is 0.1646728515625 \n",
      "Loss after 20 Batch is 0.1654052734375 \n",
      "Loss after 30 Batch is 0.16650390625 \n",
      "Loss after 40 Batch is 0.16650390625 \n",
      "Loss after 50 Batch is 0.1676025390625 \n",
      "Loss after 60 Batch is 0.1676025390625 \n",
      "Loss after 70 Batch is 0.167724609375 \n",
      "Loss after 80 Batch is 0.1680908203125 \n",
      "Loss after 90 Batch is 0.1680908203125 \n",
      "Loss after 100 Batch is 0.16796875 \n",
      "Loss after 110 Batch is 0.16748046875 \n",
      "Loss after 120 Batch is 0.1671142578125 \n",
      "Loss after 130 Batch is 0.1669921875 \n",
      "Loss after 140 Batch is 0.1668701171875 \n",
      "Loss after 150 Batch is 0.16650390625 \n",
      "Loss after 160 Batch is 0.166259765625 \n",
      "Loss after 170 Batch is 0.166259765625 \n",
      "Loss after 180 Batch is 0.1661376953125 \n",
      "Loss after 190 Batch is 0.165771484375 \n",
      "Loss after 200 Batch is 0.16552734375 \n",
      "Loss after 210 Batch is 0.1651611328125 \n",
      "Loss after 220 Batch is 0.164794921875 \n",
      "Loss after 230 Batch is 0.164306640625 \n",
      "Loss after 240 Batch is 0.1640625 \n",
      "Loss after 250 Batch is 0.163818359375 \n",
      "Loss after 260 Batch is 0.163818359375 \n",
      "Loss after 270 Batch is 0.16357421875 \n",
      "Loss after 280 Batch is 0.1634521484375 \n",
      "Loss after 290 Batch is 0.16357421875 \n",
      "Loss after 300 Batch is 0.163330078125 \n",
      "Loss after 310 Batch is 0.1630859375 \n",
      "Loss after 320 Batch is 0.162841796875 \n",
      "Loss after 330 Batch is 0.1627197265625 \n",
      "Loss after 340 Batch is 0.16259765625 \n",
      "Loss after 350 Batch is 0.162353515625 \n",
      "Loss after 360 Batch is 0.162353515625 \n",
      "Loss after 370 Batch is 0.1622314453125 \n",
      "Loss after 380 Batch is 0.162109375 \n",
      "Loss after 390 Batch is 0.161865234375 \n",
      "Loss after 400 Batch is 0.1617431640625 \n",
      "Loss after 410 Batch is 0.1614990234375 \n",
      "Loss after 420 Batch is 0.1611328125 \n",
      "Loss after 430 Batch is 0.1610107421875 \n",
      "Loss after 440 Batch is 0.1605224609375 \n",
      "Loss after 450 Batch is 0.1605224609375 \n",
      "Loss after 460 Batch is 0.16015625 \n",
      "Loss after 470 Batch is 0.15966796875 \n",
      "Loss after 480 Batch is 0.1591796875 \n",
      "Loss after 490 Batch is 0.1588134765625 \n",
      "Loss after 500 Batch is 0.158447265625 \n",
      "Loss after 510 Batch is 0.158203125 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 520 Batch is 0.1578369140625 \n",
      "Loss after 530 Batch is 0.1578369140625 \n",
      "Loss after 540 Batch is 0.157470703125 \n",
      "Loss after 550 Batch is 0.1573486328125 \n",
      "Loss after 560 Batch is 0.156982421875 \n",
      "Loss after 570 Batch is 0.15673828125 \n",
      "Loss after 580 Batch is 0.1563720703125 \n",
      "Loss after 590 Batch is 0.156005859375 \n",
      "Loss after 600 Batch is 0.155517578125 \n",
      "Loss after 610 Batch is 0.155029296875 \n",
      "Loss after 620 Batch is 0.1546630859375 \n",
      "Loss after 630 Batch is 0.1544189453125 \n",
      "Loss after 640 Batch is 0.1541748046875 \n",
      "Loss after 650 Batch is 0.154052734375 \n",
      "Loss after 660 Batch is 0.1536865234375 \n",
      "Loss after 670 Batch is 0.1531982421875 \n",
      "Loss after 680 Batch is 0.1529541015625 \n",
      "Loss after 690 Batch is 0.1527099609375 \n",
      "Loss after 700 Batch is 0.152587890625 \n",
      "Loss after 710 Batch is 0.1522216796875 \n",
      "Loss after 720 Batch is 0.15185546875 \n",
      "Loss after 730 Batch is 0.1514892578125 \n",
      "Loss after 740 Batch is 0.1512451171875 \n",
      "Loss after 750 Batch is 0.15087890625 \n",
      "Loss after 760 Batch is 0.1505126953125 \n",
      "Loss after 770 Batch is 0.150390625 \n",
      "Loss after 780 Batch is 0.150146484375 \n",
      "Loss after 790 Batch is 0.14990234375 \n",
      "Loss after 800 Batch is 0.149658203125 \n",
      "Loss after 810 Batch is 0.1492919921875 \n",
      "Loss after 820 Batch is 0.149169921875 \n",
      "Loss after 830 Batch is 0.1490478515625 \n",
      "Loss after 840 Batch is 0.14892578125 \n",
      "Loss after 850 Batch is 0.14892578125 \n",
      "Loss after 860 Batch is 0.1485595703125 \n",
      "Loss after 870 Batch is 0.1484375 \n",
      "Loss after 880 Batch is 0.1480712890625 \n",
      "Loss after 890 Batch is 0.1478271484375 \n",
      "Loss after 900 Batch is 0.1475830078125 \n",
      "Loss after 910 Batch is 0.1473388671875 \n",
      "Loss after 920 Batch is 0.1470947265625 \n",
      "Loss after 930 Batch is 0.1468505859375 \n",
      "Loss after 940 Batch is 0.1466064453125 \n",
      "Loss after 950 Batch is 0.146484375 \n",
      "Loss after 960 Batch is 0.146240234375 \n",
      "Loss after 970 Batch is 0.14599609375 \n",
      "Loss after 980 Batch is 0.145751953125 \n",
      "Loss after 990 Batch is 0.1455078125 \n",
      "Loss after 1000 Batch is 0.1453857421875 \n",
      "Loss after 1010 Batch is 0.1451416015625 \n",
      "Loss after 1020 Batch is 0.1448974609375 \n",
      "Loss after 1030 Batch is 0.144775390625 \n",
      "Loss after 1040 Batch is 0.14453125 \n",
      "Loss after 1050 Batch is 0.1444091796875 \n",
      "Loss after 1060 Batch is 0.1441650390625 \n",
      "Loss after 1070 Batch is 0.14404296875 \n",
      "Loss after 1080 Batch is 0.143798828125 \n",
      "Loss after 1090 Batch is 0.1436767578125 \n",
      "Loss after 1100 Batch is 0.1435546875 \n",
      "Loss after 1110 Batch is 0.143310546875 \n",
      "Loss after 1120 Batch is 0.1431884765625 \n",
      "Loss after 1130 Batch is 0.14306640625 \n",
      "Loss after 1140 Batch is 0.142822265625 \n",
      "Loss after 1150 Batch is 0.1427001953125 \n",
      "Loss after 1160 Batch is 0.142578125 \n",
      "Loss after 1170 Batch is 0.1424560546875 \n",
      "Loss after 1180 Batch is 0.1422119140625 \n",
      "Loss after 1190 Batch is 0.14208984375 \n",
      "Loss after 1200 Batch is 0.1419677734375 \n",
      "Loss after 1210 Batch is 0.141845703125 \n",
      "Loss after 1220 Batch is 0.1417236328125 \n",
      "Loss after 1230 Batch is 0.1416015625 \n",
      "Loss after 1240 Batch is 0.141357421875 \n",
      "Loss after 1250 Batch is 0.1412353515625 \n",
      "Loss after 1260 Batch is 0.14111328125 \n",
      "Loss after 1270 Batch is 0.1409912109375 \n",
      "Loss after 1280 Batch is 0.140869140625 \n",
      "Loss after 1290 Batch is 0.1407470703125 \n",
      "Loss after 1300 Batch is 0.140625 \n",
      "Loss after 1310 Batch is 0.1405029296875 \n",
      "Loss after 1320 Batch is 0.140380859375 \n",
      "Loss after 1330 Batch is 0.1402587890625 \n",
      "Loss after 1340 Batch is 0.14013671875 \n",
      "Loss after 1350 Batch is 0.14013671875 \n",
      "Loss after 1360 Batch is 0.1400146484375 \n",
      "Loss after 1370 Batch is 0.139892578125 \n",
      "Loss after 1380 Batch is 0.1397705078125 \n",
      "Loss after 1390 Batch is 0.1396484375 \n",
      "Loss after 1400 Batch is 0.1395263671875 \n",
      "Loss after 1410 Batch is 0.139404296875 \n",
      "Loss after 1420 Batch is 0.1392822265625 \n",
      "Loss after 1430 Batch is 0.1392822265625 \n",
      "Loss after 1440 Batch is 0.13916015625 \n",
      "Loss after 1450 Batch is 0.1390380859375 \n",
      "Loss after 1460 Batch is 0.138916015625 \n",
      "Loss after 1470 Batch is 0.1387939453125 \n",
      "Loss after 1480 Batch is 0.1387939453125 \n",
      "Loss after 1490 Batch is 0.138671875 \n",
      "Loss after 1500 Batch is 0.1385498046875 \n",
      "Loss after 1510 Batch is 0.138427734375 \n",
      "Loss after 1520 Batch is 0.138427734375 \n",
      "Loss after 1530 Batch is 0.1383056640625 \n",
      "Loss after 1540 Batch is 0.13818359375 \n",
      "Loss after 1550 Batch is 0.13818359375 \n",
      "Loss after 1560 Batch is 0.1380615234375 \n",
      "KLD-CosineLoss after 2 Epoch is 0.13803582853486884\n",
      "Loss after 0 Batch is 0.1334228515625 \n",
      "Loss after 10 Batch is 0.1348876953125 \n",
      "Loss after 20 Batch is 0.136474609375 \n",
      "Loss after 30 Batch is 0.13720703125 \n",
      "Loss after 40 Batch is 0.1358642578125 \n",
      "Loss after 50 Batch is 0.13623046875 \n",
      "Loss after 60 Batch is 0.1357421875 \n",
      "Loss after 70 Batch is 0.135498046875 \n",
      "Loss after 80 Batch is 0.1353759765625 \n",
      "Loss after 90 Batch is 0.1358642578125 \n",
      "Loss after 100 Batch is 0.1361083984375 \n",
      "Loss after 110 Batch is 0.1361083984375 \n",
      "Loss after 120 Batch is 0.135986328125 \n",
      "Loss after 130 Batch is 0.1361083984375 \n",
      "Loss after 140 Batch is 0.135498046875 \n",
      "Loss after 150 Batch is 0.1353759765625 \n",
      "Loss after 160 Batch is 0.135498046875 \n",
      "Loss after 170 Batch is 0.1357421875 \n",
      "Loss after 180 Batch is 0.1356201171875 \n",
      "Loss after 190 Batch is 0.13525390625 \n",
      "Loss after 200 Batch is 0.13525390625 \n",
      "Loss after 210 Batch is 0.1351318359375 \n",
      "Loss after 220 Batch is 0.135009765625 \n",
      "Loss after 230 Batch is 0.1348876953125 \n",
      "Loss after 240 Batch is 0.1348876953125 \n",
      "Loss after 250 Batch is 0.134521484375 \n",
      "Loss after 260 Batch is 0.1343994140625 \n",
      "Loss after 270 Batch is 0.13427734375 \n",
      "Loss after 280 Batch is 0.13427734375 \n",
      "Loss after 290 Batch is 0.134033203125 \n",
      "Loss after 310 Batch is 0.1336669921875 \n",
      "Loss after 320 Batch is 0.1337890625 \n"
     ]
    }
   ],
   "source": [
    "Trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(\n",
    "#     {\n",
    "#         \"distilbert\": student_distil_bert.state_dict(),\n",
    "#     },\n",
    "#     f\"distiled_distilbert_sst2_{datetime.now():%Y-%m-%d_%H-%M-%S%z}.pt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6ca76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
