{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87273e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import KLDivLoss, CrossEntropyLoss, CosineEmbeddingLoss, MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd6d6",
   "metadata": {},
   "source": [
    "### Loading CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88cbfadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip.model import Transformer, VisionTransformer\n",
    "from clip.model import convert_weights\n",
    "from clip.model import CLIP\n",
    "\n",
    "\n",
    "embed_dim = 512\n",
    "image_resolution = 224\n",
    "vision_layers = 12\n",
    "vision_width = 768\n",
    "vision_patch_size = 32\n",
    "context_length = 77\n",
    "vocab_size = 49408\n",
    "transformer_width = 512\n",
    "transformer_heads = 8\n",
    "transformer_layers = 12\n",
    "\n",
    "clip_model = CLIP(\n",
    "    embed_dim,\n",
    "    image_resolution,\n",
    "    vision_layers,\n",
    "    vision_width,\n",
    "    vision_patch_size,\n",
    "    context_length,\n",
    "    vocab_size,\n",
    "    transformer_width,\n",
    "    transformer_heads,\n",
    "    transformer_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948606c",
   "metadata": {},
   "source": [
    "### Convert Dataset into Torch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0ef402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dset = load_from_disk(\"./data/processed\")\n",
    "\n",
    "\n",
    "_, preprocess = clip.load(\"ViT-B/32\")\n",
    "\n",
    "\n",
    "def transform_func(examples):\n",
    "    examples[\"image\"] = [preprocess(img) for img in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "dset = dset.with_transform(transform_func)\n",
    "\n",
    "train_dataloader = DataLoader(dset, batch_size=16, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db4ba5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['caption', 'image'],\n",
       "    num_rows: 7012\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "609fd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, clip_model, train_dataloader):\n",
    "\n",
    "        self.model = clip_model\n",
    "        self.train_dataloader = train_dataloader\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.epochs = 30\n",
    "        self.start_epoch = 1\n",
    "        self.temperature = 1\n",
    "\n",
    "        self.loss_img = nn.CrossEntropyLoss()\n",
    "        self.loss_txt = nn.CrossEntropyLoss()\n",
    "        # set up optimizer\n",
    "        self.optimizer = Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=1e-3,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-6,\n",
    "            weight_decay=0.2,\n",
    "        )\n",
    "        \n",
    "    def cross_entropy(self, preds, targets, reduction=\"none\"):\n",
    "        log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        loss = (-targets * log_softmax(preds)).sum(1)\n",
    "        if reduction == \"none\":\n",
    "            return loss\n",
    "        elif reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "    \n",
    "    def compute_loss(self, images, texts, return_outputs=False):\n",
    "        texts = clip.tokenize(texts)\n",
    "        texts = texts.to(self.device)\n",
    "        images = images.to(self.device).half()\n",
    "\n",
    "        image_embeddings = self.model.encode_image(images)\n",
    "        text_embeddings = self.model.encode_text(texts)\n",
    "        \n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "\n",
    "        texts_loss = self.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        images_loss = self.cross_entropy(logits.T, targets.T, reduction=\"none\")\n",
    "        con_loss = (images_loss + texts_loss) / 2.0\n",
    "        con_loss = con_loss.mean()\n",
    "        return con_loss\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            print(f\"Starting Epoch {epoch} ------------------------------------------\")\n",
    "            loss_value = self._train_epoch(epoch)\n",
    "            print(f\"Combined Loss Value after {epoch} Epoch is {loss_value}\")\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        loss_value = 0\n",
    "        for batch_idx, data in enumerate(self.train_dataloader):\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            texts = data[\"caption\"]\n",
    "            images = data[\"image\"]\n",
    "\n",
    "            loss = self.compute_loss(images, texts)\n",
    "\n",
    "            loss_value += loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\n",
    "                    f\"Loss after {batch_idx}/{len(self.train_dataloader)} Batch is {loss_value/(batch_idx+1)} \"\n",
    "                )\n",
    "\n",
    "        return loss_value.detach().cpu().numpy() / len(self.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63c467ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = Trainer(\n",
    "    clip_model=clip_model,\n",
    "    train_dataloader=train_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c83373ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1 ------------------------------------------\n",
      "Loss after 0/439 Batch is 28.542377471923828 \n",
      "Loss after 100/439 Batch is 5.4333343505859375 \n",
      "Loss after 200/439 Batch is 3.624295473098755 \n",
      "Loss after 300/439 Batch is 2.9344849586486816 \n",
      "Loss after 400/439 Batch is 3.1431682109832764 \n",
      "Combined Loss Value after 1 Epoch is 3.00731281810578\n",
      "Starting Epoch 2 ------------------------------------------\n",
      "Loss after 0/439 Batch is 2.125880002975464 \n",
      "Loss after 100/439 Batch is 1.5105254650115967 \n",
      "Loss after 200/439 Batch is 1.5328954458236694 \n",
      "Loss after 300/439 Batch is 1.91533625125885 \n",
      "Loss after 400/439 Batch is 1.9073659181594849 \n",
      "Combined Loss Value after 2 Epoch is 1.8783043794045415\n",
      "Starting Epoch 3 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.544331431388855 \n",
      "Loss after 100/439 Batch is 1.5405009984970093 \n",
      "Loss after 200/439 Batch is 1.589935064315796 \n",
      "Loss after 300/439 Batch is 1.5900063514709473 \n",
      "Loss after 400/439 Batch is 1.5982508659362793 \n",
      "Combined Loss Value after 3 Epoch is 1.5942443986688497\n",
      "Starting Epoch 4 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.6876842975616455 \n",
      "Loss after 100/439 Batch is 1.57466459274292 \n",
      "Loss after 200/439 Batch is 1.5356051921844482 \n",
      "Loss after 300/439 Batch is 1.5279450416564941 \n",
      "Loss after 400/439 Batch is 1.5277982950210571 \n",
      "Combined Loss Value after 4 Epoch is 1.52043009347416\n",
      "Starting Epoch 5 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.7004045248031616 \n",
      "Loss after 100/439 Batch is 1.5827449560165405 \n",
      "Loss after 200/439 Batch is 1.5491523742675781 \n",
      "Loss after 300/439 Batch is 1.533617615699768 \n",
      "Loss after 400/439 Batch is 1.5177016258239746 \n",
      "Combined Loss Value after 5 Epoch is 1.514483684287799\n",
      "Starting Epoch 6 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.3935176134109497 \n",
      "Loss after 100/439 Batch is 1.5260189771652222 \n",
      "Loss after 200/439 Batch is 1.5321133136749268 \n",
      "Loss after 300/439 Batch is 1.5317860841751099 \n",
      "Loss after 400/439 Batch is 1.5227749347686768 \n",
      "Combined Loss Value after 6 Epoch is 1.5193655235620729\n",
      "Starting Epoch 7 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.5884157419204712 \n",
      "Loss after 100/439 Batch is 1.5026606321334839 \n",
      "Loss after 200/439 Batch is 1.50452721118927 \n",
      "Loss after 300/439 Batch is 1.5218818187713623 \n",
      "Loss after 400/439 Batch is 1.5334497690200806 \n",
      "Combined Loss Value after 7 Epoch is 1.526726446825171\n",
      "Starting Epoch 8 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.3867337703704834 \n",
      "Loss after 100/439 Batch is 1.50579833984375 \n",
      "Loss after 200/439 Batch is 1.5138274431228638 \n",
      "Loss after 300/439 Batch is 1.5120023488998413 \n",
      "Loss after 400/439 Batch is 1.5234785079956055 \n",
      "Combined Loss Value after 8 Epoch is 1.524048824788226\n",
      "Starting Epoch 9 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.796494483947754 \n",
      "Loss after 100/439 Batch is 1.5181350708007812 \n",
      "Loss after 200/439 Batch is 1.5311713218688965 \n",
      "Loss after 300/439 Batch is 1.5258716344833374 \n",
      "Loss after 400/439 Batch is 1.5187987089157104 \n",
      "Combined Loss Value after 9 Epoch is 1.5179069362631692\n",
      "Starting Epoch 10 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.4760078191757202 \n",
      "Loss after 100/439 Batch is 1.4904451370239258 \n",
      "Loss after 200/439 Batch is 1.5114691257476807 \n",
      "Loss after 300/439 Batch is 1.506293535232544 \n",
      "Loss after 400/439 Batch is 1.5105839967727661 \n",
      "Combined Loss Value after 10 Epoch is 1.5055934063122864\n",
      "Starting Epoch 11 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.3876348733901978 \n",
      "Loss after 100/439 Batch is 1.5193849802017212 \n",
      "Loss after 200/439 Batch is 1.502240777015686 \n",
      "Loss after 300/439 Batch is 1.5041614770889282 \n",
      "Loss after 400/439 Batch is 1.5057834386825562 \n",
      "Combined Loss Value after 11 Epoch is 1.5045270289809938\n",
      "Starting Epoch 12 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.4505287408828735 \n",
      "Loss after 100/439 Batch is 1.5382604598999023 \n",
      "Loss after 200/439 Batch is 1.5296008586883545 \n",
      "Loss after 300/439 Batch is 1.5206587314605713 \n",
      "Loss after 400/439 Batch is 1.5049550533294678 \n",
      "Combined Loss Value after 12 Epoch is 1.498591881406606\n",
      "Starting Epoch 13 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.4025667905807495 \n",
      "Loss after 100/439 Batch is 1.529823899269104 \n",
      "Loss after 200/439 Batch is 1.5210349559783936 \n",
      "Loss after 300/439 Batch is 1.5144599676132202 \n",
      "Loss after 400/439 Batch is 1.5126690864562988 \n",
      "Combined Loss Value after 13 Epoch is 1.5096749759752277\n",
      "Starting Epoch 14 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.3966925144195557 \n",
      "Loss after 100/439 Batch is 1.5125653743743896 \n",
      "Loss after 200/439 Batch is 1.501001238822937 \n",
      "Loss after 300/439 Batch is 1.5081361532211304 \n",
      "Loss after 400/439 Batch is 1.5063860416412354 \n",
      "Combined Loss Value after 14 Epoch is 1.5067393100886246\n",
      "Starting Epoch 15 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.4496922492980957 \n",
      "Loss after 100/439 Batch is 1.5088144540786743 \n",
      "Loss after 200/439 Batch is 1.5102341175079346 \n",
      "Loss after 300/439 Batch is 1.5023539066314697 \n",
      "Loss after 400/439 Batch is 1.509763240814209 \n",
      "Combined Loss Value after 15 Epoch is 1.5109835474800684\n",
      "Starting Epoch 16 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.433730125427246 \n",
      "Loss after 100/439 Batch is 1.4943631887435913 \n",
      "Loss after 200/439 Batch is 1.4937738180160522 \n",
      "Loss after 300/439 Batch is 1.4994642734527588 \n",
      "Loss after 400/439 Batch is 1.5148851871490479 \n",
      "Combined Loss Value after 16 Epoch is 1.510759149434083\n",
      "Starting Epoch 17 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.4290869235992432 \n",
      "Loss after 100/439 Batch is 1.553760051727295 \n",
      "Loss after 200/439 Batch is 1.5269454717636108 \n",
      "Loss after 300/439 Batch is 1.529986023902893 \n",
      "Loss after 400/439 Batch is 1.5313997268676758 \n",
      "Combined Loss Value after 17 Epoch is 1.5308164974551537\n",
      "Starting Epoch 18 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.6292765140533447 \n",
      "Loss after 100/439 Batch is 1.504465937614441 \n",
      "Loss after 200/439 Batch is 1.5218491554260254 \n",
      "Loss after 300/439 Batch is 1.5126476287841797 \n",
      "Loss after 400/439 Batch is 1.5139306783676147 \n",
      "Combined Loss Value after 18 Epoch is 1.5099059085368023\n",
      "Starting Epoch 19 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.38727867603302 \n",
      "Loss after 100/439 Batch is 1.5284265279769897 \n",
      "Loss after 200/439 Batch is 1.5113039016723633 \n",
      "Loss after 300/439 Batch is 1.5046805143356323 \n",
      "Loss after 400/439 Batch is 1.5083831548690796 \n",
      "Combined Loss Value after 19 Epoch is 1.5050874679669703\n",
      "Starting Epoch 20 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.5397340059280396 \n",
      "Loss after 100/439 Batch is 1.517281413078308 \n",
      "Loss after 200/439 Batch is 1.5176059007644653 \n",
      "Loss after 300/439 Batch is 1.519745111465454 \n",
      "Loss after 400/439 Batch is 1.5202034711837769 \n",
      "Combined Loss Value after 20 Epoch is 1.51974202288582\n",
      "Starting Epoch 21 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.556571364402771 \n",
      "Loss after 100/439 Batch is 1.5188013315200806 \n",
      "Loss after 200/439 Batch is 1.5228174924850464 \n",
      "Loss after 300/439 Batch is 1.50798761844635 \n",
      "Loss after 400/439 Batch is 1.5095661878585815 \n",
      "Combined Loss Value after 21 Epoch is 1.5082366873843251\n",
      "Starting Epoch 22 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.4851558208465576 \n",
      "Loss after 100/439 Batch is 1.5025153160095215 \n",
      "Loss after 200/439 Batch is 1.496679425239563 \n",
      "Loss after 300/439 Batch is 1.4931219816207886 \n",
      "Loss after 400/439 Batch is 1.4996662139892578 \n",
      "Combined Loss Value after 22 Epoch is 1.498306031107631\n",
      "Starting Epoch 23 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.6701372861862183 \n",
      "Loss after 100/439 Batch is 1.5003283023834229 \n",
      "Loss after 200/439 Batch is 1.501067876815796 \n",
      "Loss after 300/439 Batch is 1.497268795967102 \n",
      "Loss after 400/439 Batch is 1.5115833282470703 \n",
      "Combined Loss Value after 23 Epoch is 1.5136095885535308\n",
      "Starting Epoch 24 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.6940240859985352 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 100/439 Batch is 1.5146740674972534 \n",
      "Loss after 200/439 Batch is 1.512189269065857 \n",
      "Loss after 300/439 Batch is 1.5381606817245483 \n",
      "Loss after 400/439 Batch is 1.5296515226364136 \n",
      "Combined Loss Value after 24 Epoch is 1.5279656412389664\n",
      "Starting Epoch 25 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.4173530340194702 \n",
      "Loss after 100/439 Batch is 1.5520180463790894 \n",
      "Loss after 200/439 Batch is 1.5596923828125 \n",
      "Loss after 300/439 Batch is 1.5372940301895142 \n",
      "Loss after 400/439 Batch is 1.5338860750198364 \n",
      "Combined Loss Value after 25 Epoch is 1.527862757376495\n",
      "Starting Epoch 26 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.748734951019287 \n",
      "Loss after 100/439 Batch is 1.5017292499542236 \n",
      "Loss after 200/439 Batch is 1.5147569179534912 \n",
      "Loss after 300/439 Batch is 1.5090062618255615 \n",
      "Loss after 400/439 Batch is 1.5010285377502441 \n",
      "Combined Loss Value after 26 Epoch is 1.5001472351491316\n",
      "Starting Epoch 27 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.4306609630584717 \n",
      "Loss after 100/439 Batch is 1.5275472402572632 \n",
      "Loss after 200/439 Batch is 1.5189286470413208 \n",
      "Loss after 300/439 Batch is 1.512094497680664 \n",
      "Loss after 400/439 Batch is 1.5140622854232788 \n",
      "Combined Loss Value after 27 Epoch is 1.511913534177463\n",
      "Starting Epoch 28 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.3863613605499268 \n",
      "Loss after 100/439 Batch is 1.4984294176101685 \n",
      "Loss after 200/439 Batch is 1.5105303525924683 \n",
      "Loss after 300/439 Batch is 1.5047461986541748 \n",
      "Loss after 400/439 Batch is 1.5057778358459473 \n",
      "Combined Loss Value after 28 Epoch is 1.5031365674829158\n",
      "Starting Epoch 29 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.4969422817230225 \n",
      "Loss after 100/439 Batch is 1.4909744262695312 \n",
      "Loss after 200/439 Batch is 1.5012297630310059 \n",
      "Loss after 300/439 Batch is 1.5048648118972778 \n",
      "Loss after 400/439 Batch is 1.5121062994003296 \n",
      "Combined Loss Value after 29 Epoch is 1.5084570534951594\n",
      "Starting Epoch 30 ------------------------------------------\n",
      "Loss after 0/439 Batch is 1.3999680280685425 \n",
      "Loss after 100/439 Batch is 1.503859043121338 \n",
      "Loss after 200/439 Batch is 1.5040552616119385 \n",
      "Loss after 300/439 Batch is 1.5023908615112305 \n",
      "Loss after 400/439 Batch is 1.5022910833358765 \n",
      "Combined Loss Value after 30 Epoch is 1.4993425165059082\n"
     ]
    }
   ],
   "source": [
    "Trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a3c9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "format_data = \"date_%d_%m_%y_time_%H_%M_%S\"\n",
    "timestamp = datetime.strftime(datetime.now(), format_data)\n",
    "\n",
    "torch.save(Trainer.model.visual.state_dict(),f\"results/{timestamp}_OG_CLIP_VisionTransformer.pt\")\n",
    "torch.save(Trainer.model.transformer.state_dict(),f\"results/{timestamp}_OG_CLIP_TextTransformer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dbdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
