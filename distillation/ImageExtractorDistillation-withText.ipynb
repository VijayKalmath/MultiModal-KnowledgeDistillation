{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "87273e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import KLDivLoss, CrossEntropyLoss, CosineEmbeddingLoss,MSELoss\n",
    "from torch.optim import Adam,SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3cada",
   "metadata": {},
   "source": [
    "1. Teacher Image Extractor and Student Image Extractor  \n",
    "\n",
    "1. Teacher Text Extractor and Student DistilBert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b49de1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-98d6e8c39779>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-98d6e8c39779>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    loss =  (images_loss + texts_l\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "\n",
    "images_similarity = image_embeddings @ image_embeddings.T\n",
    "texts_similarity = text_embeddings @ text_embeddings.T\n",
    "\n",
    "targets = F.softmax(\n",
    "    (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    ")\n",
    "\n",
    "texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "\n",
    "images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "\n",
    "loss =  (images_loss + texts_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bd6d6",
   "metadata": {},
   "source": [
    "### Loading Teacher model ---> CLIP Image Extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "88cbfadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 87,849,216\n",
      "Input resolution: 224\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "model_name = \"ViT-B/32\"\n",
    "\n",
    "# model is the torch model.\n",
    "# preprocess function is for image preprocessing.\n",
    "\n",
    "model, preprocess = clip.load(model_name)\n",
    "\n",
    "# Get only the visual model\n",
    "teacher_model = model.visual\n",
    "input_resolution = model.visual.input_resolution\n",
    "\n",
    "print(\n",
    "    \"Model parameters:\",\n",
    "    f\"{np.sum([int(np.prod(p.shape)) for p in model.visual.parameters()]):,}\",\n",
    ")\n",
    "print(\"Input resolution:\", input_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896653c",
   "metadata": {},
   "source": [
    "### Instantiating Student model \n",
    "\n",
    "[VisionTransformer](https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/model.py#L206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a1340820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 12,044,160\n"
     ]
    }
   ],
   "source": [
    "from clip.model import VisionTransformer\n",
    "from clip.model import convert_weights # Make them float16\n",
    "# Set Student Configuration\n",
    "\n",
    "patch_size = 32\n",
    "width = 384\n",
    "layers = 6\n",
    "heads = 12\n",
    "output_dim = 512\n",
    "\n",
    "student_model = VisionTransformer(\n",
    "    input_resolution=input_resolution,\n",
    "    patch_size=patch_size,\n",
    "    width=width,\n",
    "    layers=layers,\n",
    "    heads=heads,\n",
    "    output_dim=output_dim,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "convert_weights(student_model)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Model parameters:\",\n",
    "    f\"{np.sum([int(np.prod(p.shape)) for p in student_model.parameters()]):,}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6debb0c",
   "metadata": {},
   "source": [
    "### Load the WIT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c6383d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cifar10 (/home/ecbm4040/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2ada1fb9a346fdb7da563d7d91432d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import io\n",
    "import urllib\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "\n",
    "\n",
    "def fetch_single_image(image_url, timeout=None, retries=0):\n",
    "    for _ in range(retries + 1):\n",
    "        try:\n",
    "            request = urllib.request.Request(\n",
    "                image_url,\n",
    "                data=None,\n",
    "                headers={\"user-agent\": get_datasets_user_agent()},\n",
    "            )\n",
    "            with urllib.request.urlopen(request, timeout=timeout) as req:\n",
    "                image = PIL.Image.open(io.BytesIO(req.read()))\n",
    "            break\n",
    "        except Exception:\n",
    "            image = None\n",
    "    return image\n",
    "\n",
    "\n",
    "def fetch_images(batch, num_threads, timeout=None, retries=0):\n",
    "    fetch_single_image_with_args = partial(\n",
    "        fetch_single_image, timeout=timeout, retries=retries\n",
    "    )\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        batch[\"image\"] = list(\n",
    "            executor.map(fetch_single_image_with_args, batch[\"image_url\"])\n",
    "        )\n",
    "    return batch\n",
    "\n",
    "\n",
    "num_threads = 20\n",
    "dset = load_dataset(\"cifar10\")\n",
    "# dset = dset.map(\n",
    "#     fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": num_threads}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a1997fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.RandomCrop(224),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "     transforms.Normalize((0, 0, 0), (1,1,1))])\n",
    "\n",
    "cifar100 = torchvision.datasets.CIFAR100('data/',download=True,train=True,transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(cifar100,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "609fd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.train_dataloader = train_dataloader\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.teacher = self.teacher.to(self.device)\n",
    "        self.student = self.student.to(self.device)\n",
    "        self.teacher.eval()\n",
    "\n",
    "        self.epochs = 30\n",
    "        self.start_epoch = 1\n",
    "\n",
    "        # set up optimizer\n",
    "        self.optimizer = SGD(self.student.parameters(),lr = 0.001)\n",
    "\n",
    "        # Set up LR Scheduler\n",
    "#         self.lr_scheduler = ReduceLROnPlateau(self.optimizer, \"min\")\n",
    "\n",
    "    def compute_loss(self, images, return_outputs=False):\n",
    "        images = images.to(self.device).half()\n",
    "\n",
    "        outputs_student = self.student(images)\n",
    "        \n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(images)\n",
    "#             outputs_teacher = torch.tensor(outputs_teacher.detach().cpu().numpy()).to(self.device)\n",
    "        \n",
    "\n",
    "        # assert size\n",
    "        assert outputs_student.size() == outputs_teacher.size()\n",
    "\n",
    "        # Soften probabilities and compute distillation loss\n",
    "        \n",
    "\n",
    "#         KL Divergence Loss        \n",
    "        kl_loss = KLDivLoss(reduction=\"batchmean\",log_target =True)\n",
    "        loss = kl_loss(F.log_softmax(outputs_student),F.log_softmax(outputs_teacher))\n",
    "\n",
    "        #Cosine loss\n",
    "        loss += CosineEmbeddingLoss()(\n",
    "            outputs_teacher, outputs_student, torch.ones(outputs_teacher.size()[0]).to(self.device)\n",
    "        )\n",
    "        \n",
    "#         #MSE Loss \n",
    "#         mse_loss = MSELoss()\n",
    "#         loss += mse_loss(outputs_teacher, outputs_student)\n",
    "\n",
    "\n",
    "        return  loss\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            loss_value = self._train_epoch(epoch)\n",
    "            print(f\"KLD-CosineLoss after {epoch} Epoch is {loss_value}\")\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        loss_value = 0\n",
    "        for batch_idx, (images, _) in enumerate(self.train_dataloader):\n",
    "        \n",
    "            self.optimizer.zero_grad()\n",
    "#             labels = torch.nn.functional.one_hot(labels,num_classes=100)\n",
    "\n",
    "            loss = self.compute_loss(images)\n",
    "            \n",
    "            loss_value += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "#             torch.autograd.set_detect_anomaly(True)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "#             print(\"After\",self.student.conv1.weight)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Loss after {batch_idx} Batch is {loss_value/(batch_idx+1)} \")\n",
    "\n",
    "        return loss_value.detach().cpu().numpy() / len(self.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "63c467ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = DistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    preprocess = preprocess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83373ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 Batch is 1.5751953125 \n",
      "Loss after 10 Batch is 1.4755859375 \n",
      "Loss after 20 Batch is 1.38671875 \n",
      "Loss after 30 Batch is 1.3125 \n",
      "Loss after 40 Batch is 1.2490234375 \n",
      "Loss after 50 Batch is 1.193359375 \n",
      "Loss after 60 Batch is 1.1455078125 \n",
      "Loss after 70 Batch is 1.1044921875 \n",
      "Loss after 80 Batch is 1.068359375 \n",
      "Loss after 90 Batch is 1.033203125 \n",
      "Loss after 100 Batch is 1.001953125 \n",
      "Loss after 110 Batch is 0.9736328125 \n",
      "Loss after 120 Batch is 0.9462890625 \n",
      "Loss after 130 Batch is 0.921875 \n",
      "Loss after 140 Batch is 0.89990234375 \n",
      "Loss after 150 Batch is 0.880859375 \n",
      "Loss after 160 Batch is 0.86083984375 \n",
      "Loss after 170 Batch is 0.83984375 \n",
      "Loss after 180 Batch is 0.8212890625 \n",
      "Loss after 190 Batch is 0.80419921875 \n",
      "Loss after 200 Batch is 0.7890625 \n",
      "Loss after 210 Batch is 0.775390625 \n",
      "Loss after 220 Batch is 0.76318359375 \n",
      "Loss after 230 Batch is 0.75146484375 \n",
      "Loss after 240 Batch is 0.7412109375 \n",
      "Loss after 250 Batch is 0.72900390625 \n",
      "Loss after 260 Batch is 0.71630859375 \n",
      "Loss after 270 Batch is 0.7041015625 \n",
      "Loss after 280 Batch is 0.6923828125 \n",
      "Loss after 290 Batch is 0.68115234375 \n",
      "Loss after 300 Batch is 0.6708984375 \n",
      "Loss after 310 Batch is 0.66162109375 \n",
      "Loss after 320 Batch is 0.65283203125 \n",
      "Loss after 330 Batch is 0.64404296875 \n",
      "Loss after 340 Batch is 0.63623046875 \n",
      "Loss after 350 Batch is 0.62890625 \n",
      "Loss after 360 Batch is 0.6220703125 \n",
      "Loss after 370 Batch is 0.615234375 \n",
      "Loss after 380 Batch is 0.60888671875 \n",
      "Loss after 390 Batch is 0.60302734375 \n",
      "Loss after 400 Batch is 0.59716796875 \n",
      "Loss after 410 Batch is 0.591796875 \n",
      "Loss after 420 Batch is 0.5869140625 \n",
      "Loss after 430 Batch is 0.58154296875 \n",
      "Loss after 440 Batch is 0.57666015625 \n",
      "Loss after 450 Batch is 0.5703125 \n",
      "Loss after 460 Batch is 0.5634765625 \n",
      "Loss after 470 Batch is 0.556640625 \n",
      "Loss after 480 Batch is 0.55029296875 \n",
      "Loss after 490 Batch is 0.54443359375 \n",
      "Loss after 500 Batch is 0.53857421875 \n",
      "Loss after 510 Batch is 0.53271484375 \n",
      "Loss after 520 Batch is 0.52734375 \n",
      "Loss after 530 Batch is 0.52197265625 \n",
      "Loss after 540 Batch is 0.51708984375 \n",
      "Loss after 550 Batch is 0.51220703125 \n",
      "Loss after 560 Batch is 0.5078125 \n",
      "Loss after 570 Batch is 0.5029296875 \n",
      "Loss after 580 Batch is 0.498779296875 \n",
      "Loss after 590 Batch is 0.494384765625 \n",
      "Loss after 600 Batch is 0.490478515625 \n",
      "Loss after 610 Batch is 0.486572265625 \n",
      "Loss after 620 Batch is 0.482666015625 \n",
      "Loss after 630 Batch is 0.47900390625 \n",
      "Loss after 640 Batch is 0.475341796875 \n"
     ]
    }
   ],
   "source": [
    "Trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(\n",
    "#     {\n",
    "#         \"distilbert\": student_distil_bert.state_dict(),\n",
    "#     },\n",
    "#     f\"distiled_distilbert_sst2_{datetime.now():%Y-%m-%d_%H-%M-%S%z}.pt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6ca76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
